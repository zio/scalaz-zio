"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[64077],{8293:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>l,frontMatter:()=>a,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"zio-kafka/consuming-kafka-topics-using-zio-streams","title":"Consuming Kafka topics using ZIO Streams","description":"You can stream data from Kafka using the plainStream method:","source":"@site/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md","sourceDirName":"zio-kafka","slug":"/zio-kafka/consuming-kafka-topics-using-zio-streams","permalink":"/zio-kafka/consuming-kafka-topics-using-zio-streams","draft":false,"unlisted":false,"editUrl":"https://github.com/zio/zio/edit/series/2.x/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md","tags":[],"version":"current","frontMatter":{"id":"consuming-kafka-topics-using-zio-streams","title":"Consuming Kafka topics using ZIO Streams"},"sidebar":"ecosystem-sidebar","previous":{"title":"Creating a zio-kafka Consumer","permalink":"/zio-kafka/creating-a-consumer"},"next":{"title":"Example of Consuming, Producing and Committing Offsets","permalink":"/zio-kafka/example-of-consuming-producing-and-committing-offsets"}}');var s=t(74848),o=t(28453);const a={id:"consuming-kafka-topics-using-zio-streams",title:"Consuming Kafka topics using ZIO Streams"},r=void 0,c={},m=[{value:"Consuming with more parallelism",id:"consuming-with-more-parallelism",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["You can stream data from Kafka using the ",(0,s.jsx)(n.code,{children:"plainStream"})," method:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nconsumer\n  .plainStream(Subscription.topics("topic150"), Serde.string, Serde.string) // (1)\n  .tap(cr => printLine(s"key: ${cr.record.key}, value: ${cr.record.value}")) // (2)\n  .map(_.offset) // (3)\n  .aggregateAsync(Consumer.offsetBatches) // (4)\n  .mapZIO(_.commit) // (5)\n  .runDrain\n'})}),"\n",(0,s.jsxs)(n.p,{children:["(1) There are several types of subscriptions: a set of topics, a topic name pattern, or fully specified set of\ntopic-partitions (see ",(0,s.jsx)(n.a,{href:"/zio-kafka/partition-assignment-and-offset-retrieval",children:"subscriptions"}),")."]}),"\n",(0,s.jsxs)(n.p,{children:["(2) The ",(0,s.jsx)(n.code,{children:"plainStream"})," method returns a stream of ",(0,s.jsx)(n.code,{children:"CommittableRecord"})," objects. In this line, we access the key\nand value from the ",(0,s.jsx)(n.code,{children:"CommittableRecord"})," and print them to the console."]}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["Some stream operators (like ",(0,s.jsx)(n.code,{children:"tap"})," and ",(0,s.jsx)(n.code,{children:"mapZIO"}),") break the chunking structure of the stream which heavily reduces\nthroughput. See ",(0,s.jsx)(n.a,{href:"/zio-kafka/serialization-and-deserialization#a-warning-about-mapzio",children:"a warning about mapZIO"})," for more\ninformation and alternatives."]})}),"\n",(0,s.jsx)(n.p,{children:"(3) Here we get the offset of the record, so we now have a stream of offsets."}),"\n",(0,s.jsxs)(n.p,{children:["(4) The offsets are aggregated into an ",(0,s.jsx)(n.code,{children:"OffsetBatch"}),". The offset batch keeps the highest seen offset per partition."]}),"\n",(0,s.jsxs)(n.p,{children:["(5) All offsets in the ",(0,s.jsx)(n.code,{children:"OffsetBatch"})," are committed. As soon as the commit is done, the next offset batch is pulled\nfrom the stream. Because the aggregation in (4) is asynchronous, this application is continuously committing while\nconcurrently processing records."]}),"\n",(0,s.jsx)(n.h2,{id:"consuming-with-more-parallelism",children:"Consuming with more parallelism"}),"\n",(0,s.jsxs)(n.p,{children:["To process partitions (assigned to the consumer) in parallel, you may use the ",(0,s.jsx)(n.code,{children:"partitionedStream"})," method, which creates\na nested stream of partitions:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nconsumer\n  .partitionedStream(Subscription.topics("topic150"), Serde.string, Serde.string)\n  .flatMapPar(Int.MaxValue) { case (topicPartition, partitionStream) => // (1)\n    ZStream.fromZIO(\n      printLine(s"Starting stream for topic \'${topicPartition.topic}\' partition ${topicPartition.partition}")\n    ) *>\n      partitionStream\n        .tap(record => printLine(s"key: ${record.key}, value: ${record.value}")) // (2)\n        .map(_.offset) // (3)\n  }\n  .aggregateAsync(Consumer.offsetBatches) // (4)\n  .mapZIO(_.commit)\n  .runDrain\n'})}),"\n",(0,s.jsxs)(n.p,{children:["(1) When using partitionedStream with ",(0,s.jsx)(n.code,{children:"flatMapPar(n)"}),", it is recommended to set n to ",(0,s.jsx)(n.code,{children:"Int.MaxValue"}),". N must be equal to\nor greater than the number of partitions your consumer subscribes to otherwise there will be unhandled partitions and\nKafka eventually evicts your consumer."]}),"\n",(0,s.jsxs)(n.p,{children:["Each time a new partition is assigned to the consumer, a new partition stream is created and the body of the\n",(0,s.jsx)(n.code,{children:"flatMapPar"})," is executed."]}),"\n",(0,s.jsxs)(n.p,{children:["(2) As before, here we process the received ",(0,s.jsx)(n.code,{children:"CommittableRecord"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["(3) and (4) Note how the offsets from all the sub-streams are merged into a single stream of ",(0,s.jsx)(n.code,{children:"OffsetBatch"}),"es. This\nensures only a single stream is doing commits."]}),"\n",(0,s.jsx)(n.p,{children:"In this example, each partition is processed in parallel, on separate fibers, while a single fiber is doing commits."})]})}function l(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(96540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);