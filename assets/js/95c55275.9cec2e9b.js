"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[95020],{28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var i=n(96540);const s={},o=i.createContext(s);function r(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:t},e.children)}},71052:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"zio-kafka/sharing-consumer","title":"Sharing a Consumer between multiple streams","description":"Often in a single application, you want to consume from multiple Kafka topics and process each topic in a distinct way. With zio-kafka you can use a single Consumer instance for multiple streams from different topics. It is not only easier to create a single Consumer layer instead of one for each topic, but it may be more resource efficient as well. The underlying Apache Kafka consumer, its thread pool and communication with the Kafka brokers will be shared, resulting in less resource consumption compared to when you create a Consumer instance for every topic.","source":"@site/docs/zio-kafka/sharing-consumer.md","sourceDirName":"zio-kafka","slug":"/zio-kafka/sharing-consumer","permalink":"/zio-kafka/sharing-consumer","draft":false,"unlisted":false,"editUrl":"https://github.com/zio/zio/edit/series/2.x/docs/zio-kafka/sharing-consumer.md","tags":[],"version":"current","frontMatter":{"id":"sharing-consumer","title":"Sharing a Consumer between multiple streams"},"sidebar":"ecosystem-sidebar","previous":{"title":"Preventing duplicates","permalink":"/zio-kafka/preventing-duplicates"},"next":{"title":"Serialization and Deserialization","permalink":"/zio-kafka/serialization-and-deserialization"}}');var s=n(74848),o=n(28453);const r={id:"sharing-consumer",title:"Sharing a Consumer between multiple streams"},a=void 0,c={},l=[];function u(e){const t={code:"code",em:"em",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["Often in a single application, you want to consume from multiple Kafka topics and process each topic in a distinct way. With ",(0,s.jsx)(t.code,{children:"zio-kafka"})," you can use a single ",(0,s.jsx)(t.code,{children:"Consumer"})," instance for multiple streams from different topics. It is not only easier to create a single ",(0,s.jsx)(t.code,{children:"Consumer"})," layer instead of one for each topic, but it may be more resource efficient as well. The underlying Apache Kafka consumer, its thread pool and communication with the Kafka brokers will be shared, resulting in less resource consumption compared to when you create a Consumer instance for every topic."]}),"\n",(0,s.jsxs)(t.p,{children:["For each of the topics/patterns you subscribe to, you can define a dedicated stream to process the records, a dedicated ",(0,s.jsx)(t.code,{children:"Deserializer"})," for the keys, and a dedicated ",(0,s.jsx)(t.code,{children:"Deserializer"})," for the values. Other settings like poll interval and offset strategy are common to all subscriptions. For example, the value of the ",(0,s.jsx)(t.code,{children:"max.poll.records"})," setting is the maximum number of records returned in each poll for all the subscriptions combined. If you need different settings for each topic/pattern, you need to create a ",(0,s.jsx)(t.code,{children:"Consumer"})," instance per topic/pattern."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-scala",children:'import zio._\nimport zio.Console.printLine\nimport zio.kafka.consumer._\n\n// Create a single Consumer instance\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval consumer: ZLayer[Any, Throwable, Consumer] = ZLayer.scoped(Consumer.make(consumerSettings))\n\n// Define two streams, each for a different topic and key-value types\nval stream1 = Consumer.plainStream(Subscription.topics("topic1"), Serde.string, Serde.string)\n  .tap(cr => printLine(s"For topic1, got key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n\nval stream2 = Consumer.plainStream(Subscription.topics("topic2"), Serde.uuid, Serde.int)\n  .tap(cr => printLine(s"For topic2, got key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n\n// Run both streams with the same consumer instance\n(stream1 zipPar stream2).provideSomeLayer(consumer)\n'})}),"\n",(0,s.jsx)(t.p,{children:"Each Stream can have a different lifetime. Subscriptions are tied to their Stream's scope, so when a Stream completes (successfully, via failure or interruption), the corresponding subscription is removed."}),"\n",(0,s.jsxs)(t.p,{children:["Consumer sharing is only possible when using the same type of ",(0,s.jsx)(t.code,{children:"Subscription"}),": ",(0,s.jsx)(t.code,{children:"Topics"}),", ",(0,s.jsx)(t.code,{children:"Pattern"})," or ",(0,s.jsx)(t.code,{children:"Manual"}),". Mixing different types will result in a failed stream. Note that every ",(0,s.jsx)(t.code,{children:"Topic"})," subscription can also be written as a ",(0,s.jsx)(t.code,{children:"Pattern"})," subscription, so if you want to mix ",(0,s.jsx)(t.code,{children:"Topic"})," and ",(0,s.jsx)(t.code,{children:"Pattern"})," subscriptions, you can just use ",(0,s.jsx)(t.code,{children:"Pattern"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["If your subscriptions overlap, eg ",(0,s.jsx)(t.code,{children:'Subscription.topics("topic1", "topic2")'})," and ",(0,s.jsx)(t.code,{children:'Subscription.topics("topic2", "topic3")'}),", which stream will get the partitions of ",(0,s.jsx)(t.code,{children:"topic2"})," is not deterministic."]}),"\n",(0,s.jsxs)(t.p,{children:["Note that upon starting or ending subscriptions, Kafka will also reassign all partitions of the ",(0,s.jsx)(t.em,{children:"other"})," subscriptions. Record fetching will resume from the last committed offset, so records may be processed twice when not all offsets are committed yet. This works the same as for for regular rebalancing. To minimize this issue, commit frequently."]})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);