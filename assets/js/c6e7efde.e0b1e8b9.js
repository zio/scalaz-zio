"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[63828],{28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>t});var i=r(96540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},89451:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"guides/tutorials/producing-consuming-data-from-kafka-topics","title":"Tutorial: How to Produce/Consume Data To/From Kafka Topics?","description":"Introduction","source":"@site/docs/guides/tutorials/produce-consume-data-to-from-kafka-topics.md","sourceDirName":"guides/tutorials","slug":"/guides/tutorials/producing-consuming-data-from-kafka-topics","permalink":"/guides/tutorials/producing-consuming-data-from-kafka-topics","draft":false,"unlisted":false,"editUrl":"https://github.com/zio/zio/edit/series/2.x/docs/guides/tutorials/produce-consume-data-to-from-kafka-topics.md","tags":[],"version":"current","frontMatter":{"id":"producing-consuming-data-from-kafka-topics","title":"Tutorial: How to Produce/Consume Data To/From Kafka Topics?","sidebar_label":"Producing/Consuming Data To/From Kafka Topics"},"sidebar":"guides-sidebar","previous":{"title":"Deploying a ZIO Application Using Docker","permalink":"/guides/tutorials/deploy-a-zio-application-using-docker"},"next":{"title":"Monitoring a ZIO Application Using ZIO\'s Built-in Metric System","permalink":"/guides/tutorials/monitor-a-zio-application-using-zio-built-in-metric-system"}}');var o=r(74848),s=r(28453);const a={id:"producing-consuming-data-from-kafka-topics",title:"Tutorial: How to Produce/Consume Data To/From Kafka Topics?",sidebar_label:"Producing/Consuming Data To/From Kafka Topics"},t=void 0,c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Running Examples",id:"running-examples",level:2},{value:"Adding Dependencies to The Project",id:"adding-dependencies-to-the-project",level:2},{value:"Setting Up The Kafka Cluster",id:"setting-up-the-kafka-cluster",level:2},{value:"Writing a Simple Producer and Consumer Using ZIO Workflows",id:"writing-a-simple-producer-and-consumer-using-zio-workflows",level:2},{value:"1. Serializing and Deserializing Data",id:"1-serializing-and-deserializing-data",level:3},{value:"2. Creating a Producer",id:"2-creating-a-producer",level:3},{value:"3. Producing records",id:"3-producing-records",level:3},{value:"4. Creating a Consumer",id:"4-creating-a-consumer",level:3},{value:"5. The Complete Example",id:"5-the-complete-example",level:3},{value:"Zio-kafka With Zio-streams",id:"zio-kafka-with-zio-streams",level:2},{value:"1. Streaming Producer API",id:"1-streaming-producer-api",level:3},{value:"2. Creating a Consumer",id:"2-creating-a-consumer",level:3},{value:"3. Streaming Consumer API",id:"3-streaming-consumer-api",level:3},{value:"4. The Complete Streaming Example",id:"4-the-complete-streaming-example",level:3},{value:"Producing and Consuming JSON Data",id:"producing-and-consuming-json-data",level:2},{value:"1. Writing Custom Serializer and Deserializer",id:"1-writing-custom-serializer-and-deserializer",level:3},{value:"2. The Complete JSON Streaming Example",id:"2-the-complete-json-streaming-example",level:3},{value:"Conclusion",id:"conclusion",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Kafka is a distributed, fault-tolerant, message-oriented event-store platform. It is used as a message broker for distributed applications. Zio-kafka is a library that provides a way to consume and produce data from Kafka topics, and it also supports the ability to have streaming consumers and producers."}),"\n",(0,o.jsx)(n.p,{children:"In this tutorial, we will learn how to use zio-streams and zio-kafka to produce and consume data from Kafka topics."}),"\n",(0,o.jsx)(n.h2,{id:"running-examples",children:"Running Examples"}),"\n",(0,o.jsxs)(n.p,{children:["To access the code examples, you can clone the ",(0,o.jsx)(n.a,{href:"http://github.com/zio/zio-quickstarts",children:"ZIO Quickstarts"})," project:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"$ git clone https://github.com/zio/zio-quickstarts.git\n$ cd zio-quickstarts/zio-quickstart-kafka\n"})}),"\n",(0,o.jsx)(n.p,{children:"And finally, run the application using sbt:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"$ sbt run\n"})}),"\n",(0,o.jsx)(n.h2,{id:"adding-dependencies-to-the-project",children:"Adding Dependencies to The Project"}),"\n",(0,o.jsxs)(n.p,{children:["In this tutorial, we will be using the following dependencies. So, let's add them to the ",(0,o.jsx)(n.code,{children:"build.sbt"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'libraryDependencies += Seq(\n  "dev.zio" %% "zio"         % "2.1.15",\n  "dev.zio" %% "zio-streams" % "2.1.15",\n  "dev.zio" %% "zio-kafka"   % "2.10.0",\n  "dev.zio" %% "zio-json"    % "0.7.16"\n)\n'})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zio-kafka"})," is a ZIO native client for Apache Kafka. It provides a high-level streaming API on top of the Java client. So we can produce and consume events using the declarative concurrency model of zio-streams."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zio-streams"})," introduces a high-level API for working with streams of values. It is designated to work in a highly concurrent environment. It has seamless integration with ZIO, so we have the ability to use all the features of the ZIO along with the streams, e.g. ",(0,o.jsx)(n.code,{children:"Scope"}),", ",(0,o.jsx)(n.code,{children:"Schedule"}),", ",(0,o.jsx)(n.code,{children:"ZLayer"}),", ",(0,o.jsx)(n.code,{children:"Queue"}),", ",(0,o.jsx)(n.code,{children:"Hub"})," etc. To learn more about zio-stream, we have a comprehensive section in on that ",(0,o.jsx)(n.a,{href:"/reference/stream/",children:"here"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zio-json"})," is a library to serialize and deserialize data from/to JSON data type. We will be using this library to serialize and deserialize data when reading and writing JSON data from/to Kafka topics."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"setting-up-the-kafka-cluster",children:"Setting Up The Kafka Cluster"}),"\n",(0,o.jsxs)(n.p,{children:["Before we start, we need to set up a Kafka cluster. To start a kafka cluster for testing purposes we can use the following ",(0,o.jsx)(n.code,{children:"docker-compose.yml"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-docker-compose",children:'services:\n  broker:\n    image: apache/kafka:3.9.0\n    container_name: broker\n    ports:\n      - "9092:9092"\n    environment:\n      KAFKA_NODE_ID: 1\n      KAFKA_BROKER_ID: 1\n      KAFKA_PROCESS_ROLES: broker,controller\n      KAFKA_LISTENERS: PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker:29093\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Now we can run the ",(0,o.jsx)(n.code,{children:"docker compose up -d"})," command to start the Kafka cluster:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"$ docker compose up -d\n"})}),"\n",(0,o.jsx)(n.h2,{id:"writing-a-simple-producer-and-consumer-using-zio-workflows",children:"Writing a Simple Producer and Consumer Using ZIO Workflows"}),"\n",(0,o.jsx)(n.p,{children:"To write producers and consumers, using the zio-kafka library, we have two choices:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Using ZIO workflows"}),"\n",(0,o.jsx)(n.li,{children:"Using zio-streams workflows"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In this section, we will try the first option."}),"\n",(0,o.jsx)(n.h3,{id:"1-serializing-and-deserializing-data",children:"1. Serializing and Deserializing Data"}),"\n",(0,o.jsx)(n.p,{children:"Before we can write a producer and consumer, let's talk about how data is stored in Kafka. Kafka is an event-store platform that stores records, records are key-value pairs as raw bytes. So a Kafka broker knows nothing about its records, it just appends the records to its internal log file."}),"\n",(0,o.jsxs)(n.p,{children:["To produce and consume data from Kafka, we need a way to serialize our data to a byte array and deserialize byte arrays to our data types. This is where the ",(0,o.jsx)(n.code,{children:"Serde"})," data type comes in handy. A ",(0,o.jsx)(n.code,{children:"Serde[R, A]"})," is a ",(0,o.jsx)(n.code,{children:"Serializer"})," and ",(0,o.jsx)(n.code,{children:"Deserializer"})," for values of type ",(0,o.jsx)(n.code,{children:"A"}),", which can use the environment ",(0,o.jsx)(n.code,{children:"R"})," to serialize and deserialize values."]}),"\n",(0,o.jsxs)(n.p,{children:["Here is the simplified definition of the ",(0,o.jsx)(n.code,{children:"Serde"})," data type:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"trait Serde[-R, A] {\n  def deserialize(data: Array[Byte]): RIO[R, A]\n  def serialize(value: A)           : RIO[R, Array[Byte]]\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The companion object of ",(0,o.jsx)(n.code,{children:"Serde"})," trait contains a set of built-in serializers and deserializers for primitive types:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.long"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.int"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.short"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.float"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.double"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.boolean"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.string"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.byteArray"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.byteBuffer"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"Serde.uuid"})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["In this example, the type of the ",(0,o.jsx)(n.code,{children:"key"})," is ",(0,o.jsx)(n.code,{children:"Int"})," and the type of the ",(0,o.jsx)(n.code,{children:"value"})," is ",(0,o.jsx)(n.code,{children:"String"}),". So we can use the ",(0,o.jsx)(n.code,{children:"Serde.int"})," for the ",(0,o.jsx)(n.code,{children:"key"})," and the ",(0,o.jsx)(n.code,{children:"Serde.string"})," for the ",(0,o.jsx)(n.code,{children:"value"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"2-creating-a-producer",children:"2. Creating a Producer"}),"\n",(0,o.jsxs)(n.p,{children:["Now we can create a ",(0,o.jsx)(n.code,{children:"Producer"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.kafka._\nimport zio.kafka.producer._\n\nval producer: ZIO[Scope, Throwable, Producer] =\n  Producer.make(\n    ProducerSettings(List("localhost:9092"))\n  )\n'})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"ProducerSettings"})," as constructed in this code fragment is already sufficient for many applications. However, more configuration options are available on ",(0,o.jsx)(n.code,{children:"ProducerSettings"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Notice that the created producer requires a ",(0,o.jsx)(n.code,{children:"Scope"})," in the environment. When this scope closes, the producer closes its\nconnection with the Kafka cluster. An explicit scope can be created with the ",(0,o.jsx)(n.code,{children:"ZIO.scoped"})," method."]}),"\n",(0,o.jsx)(n.h3,{id:"3-producing-records",children:"3. Producing records"}),"\n",(0,o.jsxs)(n.p,{children:["Zio-kafka has several producers that can be used to produce data on Kafka topics. In this example, we will be using the ",(0,o.jsx)(n.code,{children:"Producer.produce"})," method:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"trait Producer {\n  def produce[R, K, V](\n    topic: String,\n    key: K,\n    value: V,\n    keySerializer: Serializer[R, K],\n    valueSerializer: Serializer[R, V]\n  ): RIO[R, RecordMetadata]\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Notice how the type parameters of ",(0,o.jsx)(n.code,{children:"produce"})," method ensure that the value's type, corresponds to the type produced by the value's ",(0,o.jsx)(n.code,{children:"Serializer"})," (and the same for the key and the ",(0,o.jsx)(n.code,{children:"Serializer"})," for the key)."]}),"\n",(0,o.jsxs)(n.p,{children:["Here is a helper function that shows how we could use the ",(0,o.jsx)(n.code,{children:"produce"})," method:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"def produceRecord(producer: Producer, topic: String, key: Long, value: String): RIO[Any, RecordMetadata] =\n  producer.produce[Any, Long, String](\n    topic = topic,\n    key = key,\n    value = value,\n    keySerializer = Serde.long,\n    valueSerializer = Serde.string\n  )\n"})}),"\n",(0,o.jsx)(n.h3,{id:"4-creating-a-consumer",children:"4. Creating a Consumer"}),"\n",(0,o.jsxs)(n.p,{children:["Zio-kafka provides several ways to consume data from Kafka topics. In this example, we will use the ",(0,o.jsx)(n.code,{children:"Consumer.consumeWith"})," function."]}),"\n",(0,o.jsx)(n.p,{children:"The following helper function creates a ZIO workflow that if we run it, runs forever and consumes records from the given topic and prints them to the console, and then commits the offsets of the consumed records:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'def consumeAndPrintEvents(consumer: Consumer, groupId: String, topic: String, topics: String*): RIO[Any, Unit] =\n  consumer.consumeWith(\n    settings = ConsumerSettings(BOOSTRAP_SERVERS).withGroupId(groupId),\n    subscription = Subscription.topics(topic, topics: _*),\n    keyDeserializer = Serde.long,\n    valueDeserializer = Serde.string,\n  ) { (k, v) =>\n    Console.printLine(s"Consumed key: $k, value: $v").orDie\n  }\n'})}),"\n",(0,o.jsxs)(n.p,{children:["For performance reasons, records are always consumed in batches. The ",(0,o.jsx)(n.code,{children:"consumeWith"})," method commits the offsets of consumed records, as soon all records of a batch have been processed."]}),"\n",(0,o.jsxs)(n.p,{children:["For more options see ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/consumer-tuning",children:"consumer tuning"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"5-the-complete-example",children:"5. The Complete Example"}),"\n",(0,o.jsx)(n.p,{children:"Now it's time to combine all the above steps to create a ZIO workflow that will produce and consume data from the Kafka cluster:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\n\n/** A simple app that produces and consumes messages from a kafka cluster\n * without using ZIO Streams.\n */\nobject SimpleKafkaApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:9092")\n  private val KAFKA_TOPIC      = "hello"\n\n  def run: ZIO[Scope, Throwable, Unit] = {\n    for {\n      c <- Consumer\n        .consumeWith(\n          settings =\n            ConsumerSettings(BOOSTRAP_SERVERS).withGroupId("simple-kafka-app"),\n          subscription = Subscription.topics(KAFKA_TOPIC),\n          keyDeserializer = Serde.long,\n          valueDeserializer = Serde.string\n        ) { record =>\n          Console.printLine(s"Consumed ${record.key()}, ${record.value()}").orDie\n        }\n        .fork\n\n      producer <- Producer.make(ProducerSettings(BOOSTRAP_SERVERS))\n      p <- Clock.currentDateTime\n        .flatMap { time =>\n          producer.produce[Any, Long, String](\n            topic = KAFKA_TOPIC,\n            key = time.getHour.toLong,\n            value = s"$time -- Hello, World!",\n            keySerializer = Serde.long,\n            valueSerializer = Serde.string\n          )\n        }\n        .schedule(Schedule.spaced(1.second))\n        .fork\n\n      _ <- (c <*> p).join\n    } yield ()\n  }\n\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"zio-kafka-with-zio-streams",children:"Zio-kafka With Zio-streams"}),"\n",(0,o.jsx)(n.p,{children:"As we said before, to write producers and consumers using the zio-kafka library, we have two choices:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Using ZIO workflows"}),"\n",(0,o.jsx)(n.li,{children:"Using zio-streams workflows"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In this section we show zio-kafka's seamless integration with zio-streams."}),"\n",(0,o.jsx)(n.h3,{id:"1-streaming-producer-api",children:"1. Streaming Producer API"}),"\n",(0,o.jsxs)(n.p,{children:["With zio-kafka's ",(0,o.jsx)(n.code,{children:"Producer.produceAll"})," we get a ",(0,o.jsx)(n.code,{children:"ZPipeline"}),". It takes streams of ",(0,o.jsx)(n.code,{children:"ProducerRecord[K, V]"}),", produces these records to a Kafka topic, and then returns a stream of ",(0,o.jsx)(n.code,{children:"RecordMetadata"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"trait Producer {\n  def produceAll[R, K, V](\n    keySerializer: Serializer[R, K],\n    valueSerializer: Serializer[R, V]\n  ): ZPipeline[R, Throwable, ProducerRecord[K, V], RecordMetadata]\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Here is an example that uses ",(0,o.jsx)(n.code,{children:"Producer.produceAll"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'ZStream\n  .fromIterator(Iterator.from(0), maxChunkSize = 50)\n           // ZStream[Any, Throwable, Int]\n  .mapChunksZIO { chunk =>\n    chunk.map { i =>\n      new ProducerRecord(topic, key = i, value = s"record $i") \n    }\n  }        // ZStream[Any, Throwable, ProducerRecord]\n  .via(producer.produceAll(Serde.int, Serde.string))\n           // ZStream[Any, Throwable, RecordMetadata]\n  .runDrain\n'})}),"\n",(0,o.jsxs)(n.p,{children:["For performance reasons, method ",(0,o.jsx)(n.code,{children:"produceAll"})," produces records in batches, every chunk of the input stream results in a batch."]}),"\n",(0,o.jsx)(n.h3,{id:"2-creating-a-consumer",children:"2. Creating a Consumer"}),"\n",(0,o.jsx)(n.p,{children:"When we use the streaming API we need to construct a Consumer:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.kafka._\nimport zio.kafka.consumer._\n\nval consumer: ZIO[Scope, Throwable, Consumer] =\n  Consumer.make(\n    ConsumerSettings(List("localhost:9092"))\n      .withGroupId("streaming-kafka-app")\n  )\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Notice that the consumer requires a ",(0,o.jsx)(n.code,{children:"Scope"})," in the environment. When this scope closes, the consumer closes its\nconnection with the Kafka cluster. An explicit scope can be created with the ",(0,o.jsx)(n.code,{children:"ZIO.scoped"})," method."]}),"\n",(0,o.jsxs)(n.p,{children:["For more options see ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/creating-a-consumer",children:"creating a consumer"})," and ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/consumer-tuning",children:"consumer tuning"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"3-streaming-consumer-api",children:"3. Streaming Consumer API"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"Consumer.plainStream"})," method gives a ",(0,o.jsx)(n.code,{children:"ZStream"})," that, when run, consumes records from a Kafka topic and gives a stream of ",(0,o.jsx)(n.code,{children:"CommittableRecord[K, V]"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"trait Consumer {\n  def plainStream[R, K, V](\n    subscription: Subscription,\n    keyDeserializer: Deserializer[R, K],\n    valueDeserializer: Deserializer[R, V]\n  ): ZStream[R, Throwable, CommittableRecord[K, V]]\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Parameter ",(0,o.jsx)(n.code,{children:"subscription"})," indicates which topics and partitions should be consumed from. For example ",(0,o.jsx)(n.code,{children:'Subscription.topics("events")'})," to consume from the ",(0,o.jsx)(n.code,{children:"events"})," topic."]}),"\n",(0,o.jsxs)(n.p,{children:["Parameters ",(0,o.jsx)(n.code,{children:"keyDeserializer"})," and ",(0,o.jsx)(n.code,{children:"valueDeserializer"})," are the ",(0,o.jsx)(n.code,{children:"Serde"})," that will be used to deserialize the raw record bytes to whatever type you want them to be in."]}),"\n",(0,o.jsxs)(n.p,{children:["To indicate that a record has been consumed successfully, and make sure that no other consumer in the same group will consume this record again (even when the application crashes and restarts), we need to commit the offset of the record. This is how it works: first get the ",(0,o.jsx)(n.code,{children:"Offset"})," of the ",(0,o.jsx)(n.code,{children:"CommittableRecord"})," with the ",(0,o.jsx)(n.code,{children:"offset"})," method, then call ",(0,o.jsx)(n.code,{children:"commit"})," on the returned ",(0,o.jsx)(n.code,{children:"Offset"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"Here is an example:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.stream._\nimport zio.kafka._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval KAFKA_TOPIC = "my-topic"\nval consumer: Consumer = ???\n\nval c: ZIO[Any, Throwable, Unit] =\n  consumer\n    .plainStream(Subscription.topics(KAFKA_TOPIC), Serde.int, Serde.string)\n    .tap(e => Console.printLine(e.value))\n    .map(_.offset)      // Get the offset of the record\n    .mapZIO(_.commit)   // Commit the offset\n    .runDrain\n'})}),"\n",(0,o.jsxs)(n.p,{children:["While this works, it is not very performant. The problem with this approach is that we are committing offsets for each record separately. This causes a lot of overhead and slows down the consumption of records. To avoid this, we can aggregate offsets into batches and commit them all at once. This can be done by using the ",(0,o.jsx)(n.code,{children:"ZStream#aggregateAsync"})," along with the ",(0,o.jsx)(n.code,{children:"Consumer.offsetBatches"})," sink:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.stream._\nimport zio.kafka._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval KAFKA_TOPIC = "my-topic"\nval consumer: Consumer = ???\n\nval c: ZIO[Any, Throwable, Unit] =\n  consumer\n    .plainStream(Subscription.topics(KAFKA_TOPIC), Serde.int, Serde.string)\n    .tap(e => Console.printLine(e.value))\n    .map(_.offset)                           // Get the offset of the record\n    .aggregateAsync(Consumer.offsetBatches)  // Group offsets in an OffsetBatch\n    .mapZIO(_.commit)                        // Commit the batch of offsets\n    .runDrain\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Notice that because we are using ",(0,o.jsx)(n.code,{children:"aggregateAsync"}),", the commits run asynchronously with the upstream of ",(0,o.jsx)(n.code,{children:"aggregateAsync"}),". Every time a commit finishes, the next ",(0,o.jsx)(n.code,{children:"OffsetBatch"})," is taken and committed."]}),"\n",(0,o.jsxs)(n.admonition,{type:"caution",children:[(0,o.jsx)(n.p,{children:"Keeping the chunking structure intact is important."}),(0,o.jsxs)(n.p,{children:["In the example so far we have used ",(0,o.jsx)(n.code,{children:"tap"})," to print the records as they are consumed. Unfortunately, methods like ",(0,o.jsx)(n.code,{children:"tap"})," and ",(0,o.jsx)(n.code,{children:"mapZIO"})," destroy the chunking structure and lead to much lower throughput. Please read ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/serialization-and-deserialization#a-warning-about-mapzio",children:"a warning about mapZIO"})," for more details and alternatives."]})]}),"\n",(0,o.jsxs)(n.p,{children:["For more details see ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/consuming-kafka-topics-using-zio-streams",children:"consuming Kafka topics using ZIO Streams"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"4-the-complete-streaming-example",children:"4. The Complete Streaming Example"}),"\n",(0,o.jsx)(n.p,{children:"It's time to create a full working example of zio-kafka with zio-streams:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import org.apache.kafka.clients.producer.ProducerRecord\nimport zio._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\nimport zio.stream.ZStream\n\nobject StreamingKafkaApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:9092")\n  private val KAFKA_TOPIC      = "streaming-hello"\n\n  private val producerSettings = ProducerSettings(BOOSTRAP_SERVERS)\n  private val consumerSettings =\n    ConsumerSettings(BOOSTRAP_SERVERS).withGroupId("streaming-kafka-app")\n\n  def run: ZIO[Any, Throwable, Unit] = {\n    val p: ZIO[Any, Throwable, Unit] =\n      ZIO.scoped {\n        for {\n          producer <- Producer.make(producerSettings)\n          _ <- ZStream\n            .repeatZIO(Clock.currentDateTime)\n            .schedule(Schedule.spaced(1.second))\n            .map { time =>\n              new ProducerRecord(\n                KAFKA_TOPIC,\n                time.getMinute,\n                s"$time -- Hello, World!"\n              )\n            }\n            .via(producer.produceAll(Serde.int, Serde.string))\n            .runDrain\n        } yield ()\n      }\n\n    val c: ZIO[Any, Throwable, Unit] =\n      ZIO.scoped {\n        for {\n          consumer <- Consumer.make(consumerSettings)\n          _ <- consumer\n            .plainStream(\n              Subscription.topics(KAFKA_TOPIC),\n              Serde.int,\n              Serde.string\n            )\n            // Do not use `tap` in throughput sensitive applications because it\n            // destroys the chunking structure and leads to lower performance.\n            // See the previous section for more info.\n            .tap(r => Console.printLine("Consumed: " + r.value))\n            .map(_.offset)\n            .aggregateAsync(Consumer.offsetBatches)\n            .mapZIO(_.commit)\n            .runDrain\n        } yield ()\n      }\n\n    p <&> c\n  }\n\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"producing-and-consuming-json-data",children:"Producing and Consuming JSON Data"}),"\n",(0,o.jsxs)(n.p,{children:["Until now, we learned how to work with simple types like ",(0,o.jsx)(n.code,{children:"Int"}),", ",(0,o.jsx)(n.code,{children:"String"}),", and how to use their ",(0,o.jsx)(n.code,{children:"Serde"})," instances to serialize and deserialize the data."]}),"\n",(0,o.jsxs)(n.p,{children:["In this section, we are going to learn how to serialize/deserialize user-defined data types (like case classes), and in particular how to use the JSON format. We also will learn how to use the ",(0,o.jsx)(n.code,{children:"Serde"})," built-in instances to create more complex ",(0,o.jsx)(n.code,{children:"Serde"})," instances."]}),"\n",(0,o.jsx)(n.h3,{id:"1-writing-custom-serializer-and-deserializer",children:"1. Writing Custom Serializer and Deserializer"}),"\n",(0,o.jsxs)(n.p,{children:["In zio-kafka all the built-in serializers/deserializers are instances of the ",(0,o.jsx)(n.code,{children:"Serde"})," trait. All ",(0,o.jsx)(n.code,{children:"Serde"}),"s offers several combinators with which we can create new ",(0,o.jsx)(n.code,{children:"Serde"}),"s. We take a closer look at 2 of them:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Method ",(0,o.jsx)(n.code,{children:"inmap"})," transforms the ",(0,o.jsx)(n.code,{children:"Serde"})," with pure transformations from the source type to the target type and back."]}),"\n",(0,o.jsxs)(n.li,{children:["Method ",(0,o.jsx)(n.code,{children:"inmapZIO"})," transforms the ",(0,o.jsx)(n.code,{children:"Serde"})," with effectful transformations from the source type to the target type and back. As it accepts effectful transformations, we can encode any parsing failure with a ",(0,o.jsx)(n.code,{children:"ZIO"})," workflow."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["In this example, we use ",(0,o.jsx)(n.code,{children:"inmap"})," to transform the build-in ",(0,o.jsx)(n.code,{children:"Serde.long"})," to a ",(0,o.jsx)(n.code,{children:"Serde[Any, Instant]"}),". The ",(0,o.jsx)(n.code,{children:"inmap"})," method gets 2 pure functions, the first converts from ",(0,o.jsx)(n.code,{children:"Long"})," to ",(0,o.jsx)(n.code,{children:"Instant"}),", the second from ",(0,o.jsx)(n.code,{children:"Instant"})," to ",(0,o.jsx)(n.code,{children:"Long"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"import java.time.Instant\n\nval instantSerde: Serde[Any, Instant] =\n  Serde.long.inmap[Instant](Instant.ofEpochMilli)(_.toEpochMilli)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["In the following example, we will use ",(0,o.jsx)(n.code,{children:"inmapZIO"})," to transform the built-in ",(0,o.jsx)(n.code,{children:"Serde.string"})," to a ",(0,o.jsx)(n.code,{children:"Serde[Any, Event]"})," where ",(0,o.jsx)(n.code,{children:"Event"})," is a case class that is serialized to/deserialized from a String containing JSON."]}),"\n",(0,o.jsxs)(n.p,{children:["Let's say we have the ",(0,o.jsx)(n.code,{children:"Event"})," case class with the following fields:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"import java.time.OffsetDateTime\nimport java.util.UUID\n\ncase class Event(\n  uuid: UUID,\n  timestamp: OffsetDateTime,\n  message: String\n)\n"})}),"\n",(0,o.jsx)(n.p,{children:"First, we need to define a JSON decoder and encoder for it:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"import zio.json._\n\nobject Event {\n  implicit val encoder: JsonEncoder[Event] =\n    DeriveJsonEncoder.gen[Event]\n\n  implicit val decoder: JsonDecoder[Event] =\n    DeriveJsonDecoder.gen[Event]\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Then we need to create a ",(0,o.jsx)(n.code,{children:"Serde"})," for the ",(0,o.jsx)(n.code,{children:"Event"})," type. To convert ",(0,o.jsx)(n.code,{children:"Event"})," to JSON and back, we will use the zio-json library. To define a ",(0,o.jsx)(n.code,{children:"Serde"})," for the ",(0,o.jsx)(n.code,{children:"Event"})," type, we will use the ",(0,o.jsx)(n.code,{children:"Serde.string.inmapZIO"})," combinator:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:"import zio._\nimport zio.kafka.serde._\n\nobject EventKafkaSerde {\n  val event: Serde[Any, Event] =\n    Serde.string.inmapZIO[Any, Event](s =>\n      ZIO\n        .fromEither(s.fromJson[Event])\n        .mapError(e => new RuntimeException(e))\n    )(r => ZIO.succeed(r.toJson))\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["As we can see, we use the ",(0,o.jsx)(n.code,{children:"String#fromJson"})," to convert the string to an ",(0,o.jsx)(n.code,{children:"Event"})," object, and we also encode any parsing failure with a ",(0,o.jsx)(n.code,{children:"RuntimeException"})," in the ",(0,o.jsx)(n.code,{children:"ZIO"})," workflow."]}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/serialization-and-deserialization",children:"zio-kafka serialization and deserialization"})," for more details."]}),"\n",(0,o.jsx)(n.h3,{id:"2-the-complete-json-streaming-example",children:"2. The Complete JSON Streaming Example"}),"\n",(0,o.jsx)(n.p,{children:"Here is a full working example of producing and consuming JSON data with zio-kafka, zio-streams and zio-json:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-scala",children:'import org.apache.kafka.clients.producer.ProducerRecord\nimport zio._\nimport zio.json._\nimport zio.kafka.consumer._\nimport zio.kafka.producer.{Producer, ProducerSettings}\nimport zio.kafka.serde._\nimport zio.stream.ZStream\n\nimport java.time.OffsetDateTime\nimport java.util.UUID\n\n/** This is the data we will be sending to Kafka in JSON format. */\ncase class Event(uuid: UUID, timestamp: OffsetDateTime, message: String)\n\n/** A zio-json encoder/decoder for [[Event]]. */\nobject Event {\n  implicit val encoder: JsonEncoder[Event] =\n    DeriveJsonEncoder.gen[Event]\n\n  implicit val decoder: JsonDecoder[Event] =\n    DeriveJsonDecoder.gen[Event]\n}\n\n/** A zio-kafka serializer/deserializer for [[Event]]. */\nobject EventKafkaSerde {\n  val event: Serde[Any, Event] =\n    Serde.string.inmapZIO[Any, Event](s =>\n      ZIO\n        .fromEither(s.fromJson[Event])\n        .mapError(e => new RuntimeException(e))\n    )(r => ZIO.succeed(r.toJson))\n}\n\nobject JsonStreamingKafkaApp extends ZIOAppDefault {\n  private val BOOSTRAP_SERVERS = List("localhost:9092")\n  private val KAFKA_TOPIC      = "json-streaming-hello"\n\n  def run: ZIO[Any, Throwable, Unit] = {\n    val p: ZIO[Any, Throwable, Unit] =\n      ZIO.scoped {\n        for {\n          producer <- Producer.make(ProducerSettings(BOOSTRAP_SERVERS))\n          _ <- ZStream\n            .repeatZIO(Random.nextUUID <*> Clock.currentDateTime)\n            .schedule(Schedule.spaced(1.second))\n            .map { case (uuid, time) =>\n              new ProducerRecord(\n                KAFKA_TOPIC,\n                time.getMinute,\n                Event(uuid, time, "Hello, World!")\n              )\n            }\n            .via(producer.produceAll(Serde.int, EventKafkaSerde.event))\n            .runDrain\n        } yield ()\n      }\n\n    val c: ZIO[Any, Throwable, Unit] =\n      ZIO.scoped {\n        for {\n          consumer <- Consumer.make(\n            ConsumerSettings(BOOSTRAP_SERVERS).withGroupId("streaming-kafka-app")\n          )\n          _ <- consumer\n            .plainStream(\n              Subscription.topics(KAFKA_TOPIC),\n              Serde.int,\n              EventKafkaSerde.event\n            )\n            .tap { r =>\n              val event: Event = r.value\n              Console.printLine(\n                s"Event ${event.uuid} was sent at ${event.timestamp} with message ${event.message}"\n              )\n            }\n            .map(_.offset)\n            .aggregateAsync(Consumer.offsetBatches)\n            .mapZIO(_.commit)\n            .runDrain\n        } yield ()\n      }\n\n    p <&> c\n  }\n\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"In this tutorial we first learned how to create a producer and consumer for Kafka using the ZIO workflow with zio-kafka. Then we learned how to do the same with zio-streams. We also learned how to create a custom serializer and deserializer for the Kafka records and how to produce and consume JSON data using the zio-json library."}),"\n",(0,o.jsxs)(n.p,{children:["All the source code associated with this article is available on the ",(0,o.jsx)(n.a,{href:"http://github.com/zio/zio-quickstarts",children:"ZIO Quickstart"})," project on GitHub."]}),"\n",(0,o.jsx)(n.p,{children:"More information:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://zio.dev/zio-kafka/",children:"zio-kafka"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://zio.dev/reference/stream/",children:"zio-streams"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://zio.dev/zio-json/",children:"zio-json"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);