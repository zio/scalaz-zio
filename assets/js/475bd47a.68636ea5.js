"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[64077],{8293:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>m});const o=JSON.parse('{"id":"zio-kafka/consuming-kafka-topics-using-zio-streams","title":"Consuming Kafka topics using ZIO Streams","description":"First, create a consumer using the ConsumerSettings instance:","source":"@site/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md","sourceDirName":"zio-kafka","slug":"/zio-kafka/consuming-kafka-topics-using-zio-streams","permalink":"/zio-kafka/consuming-kafka-topics-using-zio-streams","draft":false,"unlisted":false,"editUrl":"https://github.com/zio/zio/edit/series/2.x/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md","tags":[],"version":"current","frontMatter":{"id":"consuming-kafka-topics-using-zio-streams","title":"Consuming Kafka topics using ZIO Streams"},"sidebar":"ecosystem-sidebar","previous":{"title":"Getting Started","permalink":"/zio-kafka/"},"next":{"title":"Example of Consuming, Producing and Committing Offsets","permalink":"/zio-kafka/example-of-consuming-producing-and-committing-offsets"}}');var s=t(74848),i=t(28453);const r={id:"consuming-kafka-topics-using-zio-streams",title:"Consuming Kafka topics using ZIO Streams"},a=void 0,c={},m=[];function u(e){const n={code:"code",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"First, create a consumer using the ConsumerSettings instance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio.*\nimport zio.kafka.consumer.{ Consumer, ConsumerSettings }\n\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval consumerScoped: ZIO[Scope, Throwable, Consumer] =\n  Consumer.make(consumerSettings)\nval consumer: ZLayer[Any, Throwable, Consumer] =\n  ZLayer.scoped(consumerScoped)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The consumer returned from ",(0,s.jsx)(n.code,{children:"Consumer.make"})," is wrapped in a ",(0,s.jsx)(n.code,{children:"ZLayer"}),"\nto allow for easy composition with other ZIO environment components.\nYou may provide that layer to effects that require a consumer. Here's\nan example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval data: Task[Chunk[CommittableRecord[String, String]]] =\n  Consumer.plainStream(Subscription.topics("topic"), Serde.string, Serde.string).take(50).runCollect\n    .provideSomeLayer(consumer)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["You may stream data from Kafka using the ",(0,s.jsx)(n.code,{children:"plainStream"})," method:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.plainStream(Subscription.topics("topic150"), Serde.string, Serde.string)\n  .tap(cr => printLine(s"key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n'})}),"\n",(0,s.jsxs)(n.p,{children:["To process partitions assigned to the consumer in parallel, you may use the ",(0,s.jsx)(n.code,{children:"Consumer#partitionedStream"})," method, which creates a nested stream of partitions:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.partitionedStream(Subscription.topics("topic150"), Serde.string, Serde.string)\n  .flatMapPar(Int.MaxValue) { case (topicPartition, partitionStream) =>\n    ZStream.fromZIO(printLine(s"Starting stream for topic \'${topicPartition.topic}\' partition ${topicPartition.partition}")) *>\n      partitionStream\n        .tap(record => printLine(s"key: ${record.key}, value: ${record.value}")) // Replace with a custom message handling effect\n        .map(_.offset)\n  }\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n'})}),"\n",(0,s.jsxs)(n.p,{children:["When using partitionedStream with ",(0,s.jsx)(n.code,{children:"flatMapPar(n)"}),", it is recommended to set n to ",(0,s.jsx)(n.code,{children:"Int.MaxValue"}),". N must be equal or greater than the number of partitions your consumer subscribes to otherwise there'll be unhandled partitions and Kafka will eventually evict your consumer."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(96540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);